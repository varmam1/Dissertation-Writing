\documentclass[a4paper,12pt]{article}
\usepackage{a4wide}
\usepackage{enumitem}
\usepackage{titling}
\usepackage{alltt}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{commath}
\usepackage{adjustbox}
\graphicspath{ {./images/} }
\setlength{\droptitle}{-8em}
\newcommand\tab[1][1cm]{\hspace*{#1}}

\begin{document}
	\title{Project Log on WGEBML Kin Recognition}
	\date{\vspace{-5ex}}
	\maketitle	\ 
	
\section{Related Work}

\subsection{Main Paper - WGEML}

\begin{itemize}
	\item The main parts of the paper are the face detection, the four face descriptors: LBP, HOG, SIFT, VGG, the penalty graphs and intrinsic graph and then using the graphs to figure out how the faces in the images are related. 
\end{itemize}

\section{Implementation Notes}

\subsection{Testing}
\begin{itemize}
	\item A folder for unit tests is made to correspond to each of the modules of the source code. This folder is under the src file and the test file is further subdivided into each source folder. 
	\item Unit testing is done using a combination of pytest and coverage. A make command is used to run the coverage command which references a .coveragerc file which makes sure that none of the \_\_init\_\_.py files, the venv or test files are included in the coverage report. 
\end{itemize}
\subsection{Face Detection}
\begin{itemize}
	\item Firstly, OpenCV2 was used to create a base implementation to draw a rectangle around a person's face in an image. This was done using the pre-trained classifier in ``haarcascade\_frontalface\_default.xml". This allowed us to take a file image and output another saved file image which was the original picture with a rectangle around each face. The next step is to output an image of just the face and nothing else with the same dimensions. 
	\item We were able to save the face on its own to an external image and change the dimensions of the outputted picture as needed. The current dimensions of the output is $64 \times 64$ as that is what the paper specified. 
\end{itemize}

\subsection{Feature Vectors}
\subsubsection{LBPs}

\begin{itemize}
	\item First, it was necessary to read a paper on LBPs applied to faces (Face Description with Local Binary Patterns: Application to Face Recognition). 
	\item From the paper, it was found that there were 59 labels that each pixel can belong to. It can either be uniform or non-uniform and we only cared about the uniform labels. These were values where there were only at most 2 bit transitions circularly. For example, 10000001 (2 transitions) was uniform but 10101000 (6 transitions) isn't. 
	\item It was necessary to first get the LBP value for each pixel in the image. This was done by looking at the direct neighbors of the pixel and determining if they are greater than or less than the pixel. If they were greater than the pixel, the value of that cell would be 1, otherwise it would be 0. Then, the value of the pixel in question was determined by looking at the pixel to the left and going counterclockwise and creating the bit string. In the following example:\\
	
	\adjustbox{center}{
	\includegraphics[scale=0.3]{LBP_help}
	}
	
	And so the value for the pixel becomes $01011100_2 = 92$. This value isn't uniform so this would have been marked as $-1$ in the process to mark it as non-uniform. This is done with every pixel in the image. For the pixels on the border, a neighborhood of size $3 \times 3 $ was still taken but any ``neighbors" that weren't in the image were assumed to be 0. So, for the top pixels, the 3 pixels above it were assumed to be 0, for example. 
	\item After getting the LBP value for each pixel, the face image is split into $8 \times 8$ rectangular blocks and the vector is computed in each block. The vector is just a histogram of the values that each pixel could have been. Since there are 58 uniform values and 1 for any non-uniform values, there were 59 values that the pixels could have taken so the vector for each block would correspond to:
	\[[\text{count}(-1), \text{count}(0), ..., \text{count}(255)]\]
	Where the uniform values are ordered in ascending order. 
	
	The vectors for each block are then concatenated to each other where the top left block is first and then the block to the right of it and so on going row by row. This outputs the 3776 dimensional vector for each face image for a $64 \times 64$ image. 
	
\end{itemize}

\subsubsection{Histogram of Gradients}

\begin{itemize}
	\item First, to compute the gradients of the image, the Sobel operator was used. To approximate the gradient in the $x$ direction, first the kernel:
	\[G_x = \begin{bmatrix}
	    -1 & 0 & 1  \\
	    -2 & 0 & 2  \\
	    -1 & 0 & 1
	\end{bmatrix}
	\]
	Was convolved on the image and then to get the gradient in the $y$ direction, the kernel:
	\[G_y = \begin{bmatrix}
	  -1 & -2 & -1  \\
	  0 & 0 & 0  \\
	  1 & 2 & 1
	\end{bmatrix}
	\]
	Was convolved on the image. This let us get the gradients in both the $x$ and $y$ direction for each pixel and then that was converted into magnitude and angle. For each pixel, the maximum magnitude and maximum angle for the 3 channels was taken to be the magnitude and angle for that pixel. For example, if a pixel had magnitudes $(1, 2, 3)$ for the $(R, G, B)$ channels, then $3$ would be the magnitude of that pixel. We also required that the angles be unsigned, so they must be between $0^\circ$ and $180^\circ$. 
	\item Once the magnitudes and the unsigned angles are obtained for the image, the image is split up into blocks of size $n \times n$, in which in our case, it is first $16 \times 16$ then $8 \times 8$. For each block, a 9-dimensional vector is obtained which is the histogram of angles for that block. The labels of the histogram are the angles:
	\[[0, 20, 40, 60, 80, 100, 120, 140, 160]\]
	So if a gradient has angle $0^\circ$, then it would count towards the first bin. Given a pixel with gradient with magnitude $m$ and angle $\theta$, if $\theta$ is one of the labels, then you would add $m$ to the bar with label $\theta$. For example, if $\theta = 0$, then vec[0] += $m$. If $\theta$ is between labels $\phi_1$ and $\phi_2$, then vec[$\phi_1$] += $\displaystyle \frac{\phi_2 - \theta}{20} \cdot m$ and vec[$\phi_2$] += $\displaystyle \frac{\theta - \phi_1}{20} \cdot m$. In other words, the amount that goes to each label is weighted with respect to the magnitude of the gradient and how close to the labels the angle of the gradient is. 
	\item The vectors for each of the 256 blocks are created for when there are $16 \times 16$ blocks and then for the 64 blocks for when there are $8 \times 8$ blocks. The paper then doesn't seem to normalize the vectors so \textbf{that is a potential improvement on the algorithm} as normalization tends to improve performance. 
	\item For a $64 \times 64$ face image, the vector that will come out of it will be a 2880-dimensional vector. The vectors for the $16 \times 16$ blocks are concatenated first and then the ones for the $8 \times 8$ blocks.
	\item Much of the information from this comes from the paper ``Histograms of Oriented Gradients for Human Detection"
\end{itemize}

\subsubsection{SIFT}
\begin{itemize}
	\item Read the paper "Distinctive Image Features from Scale-Invariant Keypoints" which was what introduced the SIFT algorithm. 
\end{itemize}
\subsubsection{VGG}


\end{document}